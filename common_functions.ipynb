{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "common_functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOG2JS+2heqAQL9dyxN6Azr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "KD53j9PLkYvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score(model, X_test, y_test):   \n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print('Recall: %.3f \\nPrecision: %.3f\\nF1: %.3f \\nAccuracy: %.3f \\nROC AUC: %.3f' % \\\n",
        "        (recall_score(y_test, y_pred),\n",
        "        precision_score(y_test, y_pred), \n",
        "        f1_score(y_test, y_pred), \n",
        "        accuracy_score(y_test, y_pred),\n",
        "        roc_auc_score(y_test, model.predict_proba(X_test)[:,1])))"
      ],
      "metadata": {
        "id": "uWo6FB6psP2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_and_score(model, X, y, X_test, y_test):\n",
        "    model.fit(X, y)\n",
        "    \n",
        "    score(model, X_test, y_test)"
      ],
      "metadata": {
        "id": "-7_Nroz7iGkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_diagram_simple(y_test, y_pred, print_percents=True):\n",
        "  confusion = confusion_matrix(y_test, y_pred)\n",
        "  plt.figure(dpi=100)\n",
        "  palette = sns.color_palette(\"ch:start=0.1,rot=-0.1\", as_cmap=True)\n",
        "  ax = sns.heatmap(confusion, cmap=palette, annot=True, square=True, fmt='d', cbar=False, linewidths=0.1, linecolor='grey',\n",
        "            xticklabels=['no claim', 'claim'],\n",
        "            yticklabels=['no claim', 'claim']);\n",
        "  ax.spines['top'].set_color('grey')\n",
        "  ax.spines['top'].set_visible(True) \n",
        "  ax.spines['right'].set_color('grey')\n",
        "  ax.spines['right'].set_visible(True) \n",
        "  plt.xlabel('prediction')\n",
        "  plt.ylabel('actual')\n",
        "  plt.title('confusion matrix \\n')\n",
        "\n",
        "  if print_percents:\n",
        "    group_percents(confusion)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "i2Rmu0OC6yyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_percents(confusion):\n",
        "    if 0 not in confusion:\n",
        "      no_claim_pred_percent = confusion[1][0] / (confusion[0][0] + confusion[1][0]) * 100\n",
        "      no_claim_pred_percent\n",
        "\n",
        "      claim_pred_percent = confusion[1][1] / (confusion[0][1] + confusion[1][1]) * 100\n",
        "      claim_pred_percent\n",
        "\n",
        "      print('Percent claims in predicted no claim group: %.2f%% \\n\\\n",
        "Percent claims in predicted claim group: %.2f%% \\n' % (no_claim_pred_percent,claim_pred_percent))\n",
        "         \n",
        "      payout_per_low_risk_member = confusion[1][0] * 7000 / (confusion[0][0] + confusion[1][0])\n",
        "      payout_per_high_risk_member = confusion[1][1] * 7000 / (confusion[0][1] + confusion[1][1])\n",
        "\n",
        "      print('Av payout per low risk member: $%.2f \\n\\\n",
        "Av payout per high risk member: $%.2f \\n' % (payout_per_low_risk_member,payout_per_high_risk_member))\n",
        "      \n",
        "      print('Total Policies: %.2f \\nTotal Claims: %.2f \\nOverall Percent claims %.2f%%\\n\\n' % \\\n",
        "      (np.sum(confusion),\n",
        "      np.sum(confusion[1]),\n",
        "      np.sum(confusion[1]) / np.sum(confusion) * 100 ))"
      ],
      "metadata": {
        "id": "ZymQVStutvu7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H97wFHZBVrhk"
      },
      "outputs": [],
      "source": [
        "def confusion_diagram(X_test, y_test, model):\n",
        "  y_pred = model.predict(X_test)\n",
        "  \n",
        "  confusion_diagram_simple(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_recall_diagram(X_test, y_test, model):\n",
        "  precision_curve, recall_curve, threshold_curve = precision_recall_curve(y_test, model.predict_proba(X_test)[:,1] )\n",
        "\n",
        "  plt.figure(dpi=100)\n",
        "  plt.plot(threshold_curve, precision_curve[1:],label='precision')\n",
        "  plt.plot(threshold_curve, recall_curve[1:], label='recall')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.xlabel('Threshold (above this probability, label as \\'filed claim\\')');\n",
        "  plt.title('Precision and Recall Curves');"
      ],
      "metadata": {
        "id": "QGG4sbyxnSqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_diagram(X_test, y_test, model):\n",
        "  fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
        "\n",
        "  plt.plot(fpr, tpr,lw=2)\n",
        "  plt.plot([0,1],[0,1],c='violet',ls='--')\n",
        "  plt.xlim([-0.05,1.05])\n",
        "  plt.ylim([-0.05,1.05])\n",
        "\n",
        "  plt.xlabel('False positive rate')\n",
        "  plt.ylabel('True positive rate')\n",
        "  plt.title('ROC curve for predicting claims');\n",
        "  print(\"ROC AUC score = \", roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))"
      ],
      "metadata": {
        "id": "dzu-4PXRj-UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def threshold_diagram(X_test, y_test, model):\n",
        "  thresh_ps = np.linspace(0,1,1000)\n",
        "  model_test_probs = model.predict_proba(X_test)[:,1]\n",
        "  f1_scores, prec_scores, rec_scores, acc_scores = [], [], [], []\n",
        "  for p in thresh_ps:\n",
        "      model_val_labels = model_test_probs >= p\n",
        "      f1_scores.append(f1_score(y_test, model_val_labels))    \n",
        "      prec_scores.append(precision_score(y_test, model_val_labels))\n",
        "      rec_scores.append(recall_score(y_test, model_val_labels))\n",
        "      acc_scores.append(accuracy_score(y_test, model_val_labels))\n",
        "      \n",
        "  plt.plot(thresh_ps, f1_scores)\n",
        "  plt.plot(thresh_ps, prec_scores)\n",
        "  plt.plot(thresh_ps, rec_scores)\n",
        "  plt.plot(thresh_ps, acc_scores)\n",
        "\n",
        "  plt.title('Metric Scores vs. Positive Class Decision Probability Threshold')\n",
        "  plt.legend(['F1','Precision','Recall','Accuracy'], bbox_to_anchor=(1.05, 0), loc='lower left')\n",
        "  plt.xlabel('P threshold')\n",
        "  plt.ylabel('Metric score')\n",
        "\n",
        "  best_f1_score = np.max(f1_scores) \n",
        "  best_thresh_p = thresh_ps[np.argmax(f1_scores)]\n",
        "\n",
        "  print('Best F1 score %.3f at prob decision threshold >= %.3f' \n",
        "        % (best_f1_score, best_thresh_p))"
      ],
      "metadata": {
        "id": "dnm3QowPx59u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_gini(y_true, y_prob):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_true = y_true[np.argsort(y_prob)]\n",
        "    ntrue = 0\n",
        "    gini = 0\n",
        "    delta = 0\n",
        "    n = len(y_true)\n",
        "    for i in range(n-1, -1, -1):\n",
        "        y_i = y_true[i]\n",
        "        ntrue += y_i\n",
        "        gini += y_i * delta\n",
        "        delta += 1 - y_i\n",
        "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
        "    return gini"
      ],
      "metadata": {
        "id": "D_19cu4RxU87"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}